{"cells":[{"metadata":{"dc":{"key":"4"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 1. This... is... Jeopardy!\n<p><img src=\"https://assets.datacamp.com/production/project_796/img/jeopardy_logo.png\" alt></p>\n<p>\"This… is… <em>Jeopardy!</em>\" These words will ring a bell for anyone who has watched the American game show, <em>Jeopardy!</em> . This iconic TV show could be described as quizbowl with gambling. In each 30-minute episode, three contestants compete in answering questions with specific monetary value, accumulating and wagering their earnings throughout each round. It's no petty cash, either; a recent <em>Jeopardy!</em> champion, James Holzhauer, walked away with $2.46 million after winning in 33 consecutive episodes. </p>\n<p>However, for an aspiring <em>Jeopardy!</em> champion, the amount of knowledge required to excel at the game might seem discouraging at first glance. How can it be possible to know everything about everything? Some <em>Jeopardy!</em> enthusiasts have turned to data analysis for the answers&mdash;and we'll do just that. In this project, we'll use basic text mining techniques on <em>Jeopardy!</em> data to spot trends in the types of questions asked. Let's start by loading in the dataset and the packages we'll need. </p>"},{"metadata":{"dc":{"key":"4"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Load packages\nlibrary(readr)\nlibrary(dplyr)\nlibrary(tm)\nlibrary(wordcloud)\n\n# Load the dataset\njeopardy <- read_csv(\"datasets/jeopardy.csv\")","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"12"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 2. A glimpse ahead\n<p><img src=\"https://assets.datacamp.com/production/project_796/img/Jeopardy_game_board.png\" alt></p>\n<p>Here are the basic rules of the game. Three contestants compete against each other in three rounds: <em><strong>Jeopardy</strong></em>, <em><strong>Double Jeopardy</strong></em>, and <em><strong>Final Jeopardy</strong></em>. In <em>Jeopardy</em> and <em>Double Jeopardy</em> , each round has six categories, with five answers per category. After an answer is read by the show's host, Alex Trebeck, each contestant competes to be the first to come up with the correct question to the answer. </p>\n<p>Each answer has a monetary value based on its difficulty. The monetary values in the <em>Double Jeopardy</em> round are double the values of the answers in <em>Jeopardy</em> round. In <em>Final Jeopardy</em>, the contestants bet any amount from their accumulated earnings on one difficult answer. </p>\n<p>For a complete breakdown of the rules, check out the <a href=\"https://en.wikipedia.org/wiki/Jeopardy!#Gameplay\"><em>Jeopardy!</em> Wikipedia page</a>. Knowing the rules of the game will make the <code>jeopardy</code> dataset easier to understand!</p>"},{"metadata":{"dc":{"key":"12"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Glimpse the dataset\nglimpse(jeopardy)\n\n# Display the first six rows\nhead(jeopardy, n = 6)","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"19"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 3. Corpus of categories\n<p>Whew. Where do we even start? <em>Jeopardy!</em> questions and answers include all kinds of words&mdash;places, people, even obscure vocabulary. Did you know that \"philately\" means \"love of stamp collecting?\" Check out the data from show number 6108. </p>\n<p>It might be better to start with a small-scale text analysis. Let's look at the <strong>categories</strong>. In addition to having clever and sometimes downright funny names, they'll tell us a little more about the content of the questions without having to analyze the question text. </p>\n<p>We need to take the categories data and convert it to an easily-workable body of text&mdash;in other words, a corpus.</p>"},{"metadata":{"dc":{"key":"19"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Create the categories variable\ncategories <- jeopardy %>%\n  filter(round == \"Jeopardy!\") %>%\n  select(category)\n\n# Create a vector source\ncategories_source <- VectorSource(categories)\n\n# Create a corpus from the vector source\ncategories_corp <- VCorpus(categories_source)","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"26"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 4. Cleaning the categories\n<p><em>Jeopardy!</em> categories are notorious for being witty and unique. An example of a category title is \"Element, Spel-ement\" (from the episode aired on March 28, 2011). Every question in this category gave the contestant a list of chemical element names, and the contestant had to spell the word created by the symbols of those elements (example: boron, aluminum, potassium = \"balk\"). </p>\n<p>Some categories have more straightforward titles, such as the \"Indonesia\" category (from the episode on March 25, 2011), which had questions all about Indonesia.</p>\n<p>You can imagine that text mining from these wordy and specific categories might be difficult&mdash;and this would probably be correct. Some cleaning is in order! Bust out the vacuums (or in this case, some <code>tm</code> package verbs).</p>"},{"metadata":{"dc":{"key":"26"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Clean the corpus\nclean_corp <- tm_map(categories_corp, content_transformer(tolower))\nclean_corp <- tm_map(clean_corp, removePunctuation)\nclean_corp <- tm_map(clean_corp, stripWhitespace)\nclean_corp <- tm_map(clean_corp, removeWords, stopwords(\"en\"))\n\n# Create a TDM from the clean corpus\ncategories_tdm <- TermDocumentMatrix(clean_corp)","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"33"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 5. Favorite topics\n<p>A basic, yet fairly effective analysis here would be a word-frequency analysis. If certain words popped on in category titles more often than others, we could reasonably assume that there are recurring themes in <em>Jeopardy!</em> categories. </p>\n<p>First, we will need to turn the TDM into an M (a matrix). Then, we will rank the most frequent words.</p>"},{"metadata":{"dc":{"key":"33"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Create a matrix from the TDM\ncategories_m <- as.matrix(categories_tdm)    \n\n# Sum the values in each row and sort them in decreasing order\nterm_frequency <- sort(rowSums(categories_m), decreasing = TRUE)\n\n# Barplot of the twelve most frequent words\nbarplot(term_frequency[1:12], col = \"orange\", las = 2)","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"40"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 6. Removing unwanted words\n<p>That is a nice barplot…but we're in this for the money and <em>Jeopardy!</em> fame. Let's improve the bar plot by removing some unhelpful words: \"time,\" \"new,\" \"first,\" and \"lets.\"</p>"},{"metadata":{"dc":{"key":"40"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Remove additional words from the corpus\ncleaner_corp <- tm_map(clean_corp, removeWords, \n                        c(stopwords(\"en\"), \"time\", \"new\", \"first\", \"lets\")) \n# Create a TDM \ncleaner_tdm <- TermDocumentMatrix(cleaner_corp)\n\n# Copy your code from Task 5 (change barplot colors if you want)\ncategories_m <- as.matrix(cleaner_tdm)    \nterm_frequency <- sort(rowSums(categories_m), decreasing = TRUE)\nbarplot(term_frequency[1:12], col = \"honeydew3\", las = 2)","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"47"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 7. Creating better tools, part 1\n<p>A few of our top ranking category words are: \"words,\" \"world,\" \"state,\" \"name,\" and \"history.\" </p>\n<p>\"Words\" most likely refers to the wordplay or vocabulary categories, which appear often on the show. The other four words suggest that a <em>Jeopardy!</em> champion will need to know a lot about history, geography, and significant historical figures. However, when we go further down the plot, there's an interesting term&mdash;the 11th most common term is \"American.\" Considering this is an American game show, it would make sense that the game requires the contestants to be <strong>most</strong> familiar with American history. We should look into this!</p>\n<p>But first, let's save some time by condensing many lines of code into one. We'll write simple, one-line functions for the cleaning and term-frequency extraction processes.</p>"},{"metadata":{"dc":{"key":"47"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Create a cleaning function\nspeed_clean <- function(corpus){\n  corpus <- tm_map(corpus, content_transformer(tolower))\n  corpus <- tm_map(corpus, stripWhitespace)\n  corpus <- tm_map(corpus, removePunctuation)\n  corpus <- tm_map(corpus, removeWords, stopwords(\"en\"))\n  return(corpus)\n}","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"54"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 8. Creating better tools, part 2\n<p>We can incorporate the <code>speed_clean()</code> function we just made into a new function that will extract frequent terms from an already-cleaned matrix. Then, we'll move on to <strong>Final Jeopardy</strong>, the last and toughest round (also, the one with the iconic <a href=\"https://www.youtube.com/watch?v=73tGe3JE5IU\"><em>Jeopardy!</em> song</a>).</p>"},{"metadata":{"dc":{"key":"54"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# Create freq_terms function\nfreq_terms <- function(list) {\n  source <- VectorSource(list)\n  corpus <- VCorpus(source)\n  clean_corpus <- speed_clean(corpus)\n  tdm <- TermDocumentMatrix(clean_corpus)\n  matrix <- as.matrix(tdm)\n  term_frequency <- sort(rowSums(matrix), decreasing = TRUE)\n  return(term_frequency)\n}","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"61"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 9. Think!\n<p><strong>Final Jeopardy</strong> is arguably the most important round in the entire game&mdash;contestants bet any amount from their accumulated earnings on one answer. This answer is supposedly more difficult than all the questions in the previous rounds. The contestants make their bets before the answer is read and are given 30 seconds to write down their questions. You can probably imagine how much of a game-changer this round is (check out <a href=\"https://www.youtube.com/watch?v=SN2hQZWwOCU\">this</a> for proof). </p>\n<p>Since we've already looked at the categories, let's look at some of the correct answers to <strong>Final Jeopardy</strong> questions.</p>"},{"metadata":{"dc":{"key":"61"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# ... YOUR CODE FOR TASK 9 ... ","execution_count":0,"outputs":[]},{"metadata":{"dc":{"key":"68"},"deletable":false,"editable":false,"run_control":{"frozen":true},"tags":["context"]},"cell_type":"markdown","source":"## 10. A few insights\n<p>John, William, James, and Henry… who might these people be? We don't know exactly, but the wordcloud seems to support and expand upon a hunch we had a little while ago - many <em>Jeopardy!</em> questions are drawn from American or European history. While it is certainly possible to get a category like \"Indonesia,\" contestants are much more likely to be tested on the history, literature, or pop culture from the west. This might not be surprising, but there are plenty of other insights to be drawn from the dataset using the text mining techniques we have explored in this project.</p>\n<p>As an aspiring <em>Jeopardy!</em> champion, which textbook might best help you study?</p>\n<ul>\n<li><p>a. \"Geography of Indonesia\"  </p></li>\n<li><p>b. \"Chemistry 101\"  </p></li>\n<li><p>c. \"U.S. History\"  </p></li>\n<li><p>d. \"Introduction to Text Mining in R\"  </p></li>\n</ul>"},{"metadata":{"dc":{"key":"68"},"tags":["sample_code"],"trusted":false,"collapsed":true},"cell_type":"code","source":"# ... YOUR CODE FOR TASK 10 ... ","execution_count":0,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.5.3"}},"nbformat":4,"nbformat_minor":2}